# Best-Parts Map

| Layer | Best LLM for This Layer | What to Extract (copy the key paragraph/section) |
|-------|------------------------|--------------------------------------------------|
| Layer 1: Data Foundation        |   Claude       |      Each notebook is a strict data isolation unit. Sources uploaded to Notebook A are never accessible to Notebook B, even within the same user account. At the infrastructure level this likely means per-notebook embedding namespaces rather than a shared multi-tenant vector index. Google has explicitly stated that user content is not used to train public models (without consent), implying a tenant-boundary enforcement layer before any telemetry flows to training pipelines. Encryption at rest (CMEK available for Workspace enterprise users) and in transit is standard GCP posture.       |
| Layer 2: Statistics & Analysis  |    Claude      |         Google uses an internal experimentation platform (overlapping with what powers Search and other products) rather than off-the-shelf tools like Optimizely. Feature flags gate new capabilities — for example, new source types (web URLs were added post-launch) and Audio Overview rolled out via staged flag. A/B tests would measure answer quality proxies (citation usage, follow-up query rates as a dissatisfaction signal) rather than simple click metrics.   |
| Layer 3: ML Models              |   Claude          |     Largely not applicable in the traditional sense. NotebookLM doesn't recommend content from a catalog — it's a closed-corpus tool. There's no "you might also like" surface. The closest analog is suggested follow-up questions generated post-response, but those are LLM-generated, not a separate ML ranking model.        |
| Layer 4: LLM / Generative AI   |    Claude      |       The underlying model is Gemini 1.5 Pro / Gemini 2.0 — this is confirmed by Google. Gemini's extremely long context window (1M tokens) is architecturally significant: it allows injecting entire small-to-medium documents directly into context rather than chunking aggressively, which reduces retrieval errors. For large notebooks, hybrid chunked retrieval + long context is used.      |
| Layer 5: Deployment & Infra     |      Claude    |        NotebookLM's collaboration is not real-time co-editing (not Google Docs-level). Shared notebooks have eventual consistency — if two users edit notes simultaneously, standard last-write-wins or simple OT may apply. Full CRDTs are not applicable here. The AI chat is per-user session, not shared. WebSockets or Server-Sent Events (SSE) are used for streaming LLM responses to the browser.     |
| Layer 6: System Design & Scale  |    Claude      |         The per-notebook vector index isolation model is elegant but creates scaling challenges. At millions of notebooks, you can't create a dedicated ANN index per notebook — the overhead is prohibitive. The likely solution is namespace-based partitioning within shared Vertex AI Vector Search indexes, with notebook_id as a mandatory filter predicate applied at query time. This allows shared index infrastructure while maintaining data isolation. Embeddings are stored with notebook_id metadata, and ANN search filters by namespace before computing similarity.        |
| Overall Analysis / Hardest Problem |    Claude   |        It explained the concept clearly and also mentioned the actual technologies used. This made the response more practical and closer to real-world applications. The inclusion of real technical references helped in understanding how the concept works in practice. It also made the explanation more credible and useful for learning.  |
| Writing Style / Structure       |    Claude      |       It explained the concept clearly and also mentioned the actual technologies used.      |
